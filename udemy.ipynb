{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Udemy\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data & Peeking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/d:/Subhrajyoti/Self Study/pyspark/nyc-taxi-trip-duration/train.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17876\\3764734198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df = spark.read.csv(r'.\\nyc-taxi-trip-duration\\train.csv',\n\u001b[0m\u001b[0;32m      2\u001b[0m                     inferSchema=True, header=True)\n",
      "\u001b[1;32mc:\\users\\snn325949\\anaconda3_\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\snn325949\\anaconda3_\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\snn325949\\anaconda3_\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/d:/Subhrajyoti/Self Study/pyspark/nyc-taxi-trip-duration/train.csv."
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(r'.\\nyc-taxi-trip-duration\\train.csv',\n",
    "                    inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|id2875421|        2|2016-03-14 17:24:55|2016-03-14 17:32:30|              1| -73.9821548461914| 40.76793670654297|-73.96463012695312|40.765602111816406|                 N|          455|\n",
      "|id2377394|        1|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|-73.98041534423828|40.738563537597656|-73.99948120117188| 40.73115158081055|                 N|          663|\n",
      "|id3858529|        2|2016-01-19 11:35:24|2016-01-19 12:10:48|              1| -73.9790267944336|40.763938903808594|-74.00533294677734|40.710086822509766|                 N|         2124|\n",
      "|id3504673|        2|2016-04-06 19:32:31|2016-04-06 19:39:40|              1|-74.01004028320312|   40.719970703125|-74.01226806640625| 40.70671844482422|                 N|          429|\n",
      "|id2181028|        2|2016-03-26 13:30:55|2016-03-26 13:38:10|              1|-73.97305297851562|40.793209075927734| -73.9729232788086| 40.78252029418945|                 N|          435|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------------+------------------+-------------------+--------------------+-------------------+-------------------+------------------+------------------+\n",
      "|summary|       id|          vendor_id|   passenger_count|   pickup_longitude|     pickup_latitude|  dropoff_longitude|   dropoff_latitude|store_and_fwd_flag|     trip_duration|\n",
      "+-------+---------+-------------------+------------------+-------------------+--------------------+-------------------+-------------------+------------------+------------------+\n",
      "|  count|  1458644|            1458644|           1458644|            1458644|             1458644|            1458644|            1458644|           1458644|           1458644|\n",
      "|   mean|     NULL| 1.5349502688798637|1.6645295219395548| -73.97348630489282|  40.750920908391734|  -73.9734159469458|   40.7517995149002|              NULL| 959.4922729603659|\n",
      "| stddev|     NULL|0.49877715390740074| 1.314242167823115|0.07090185842270368|0.032881186257633095|0.07064326809720285|0.03589055560563716|              NULL|5237.4317244975955|\n",
      "|    min|id0000001|                  1|                 0|-121.93334197998047|   34.35969543457031|-121.93330383300781|   32.1811408996582|                 N|                 1|\n",
      "|    max|id4000000|                  2|                 9| -61.33552932739258|   51.88108444213867| -61.33552932739258|  43.92102813720703|                 Y|           3526282|\n",
      "+-------+---------+-------------------+------------------+-------------------+--------------------+-------------------+-------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField,StringType,\n",
    "                               IntegerType,StructType,\n",
    "                               TimestampType,DoubleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes  =   [StringType(),StringType(),TimestampType(),TimestampType(),\n",
    "            IntegerType(),DoubleType(),DoubleType(),\n",
    "            DoubleType(),DoubleType(),StringType(),IntegerType()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField('id', StringType(), True),\n",
       " StructField('vendor_id', StringType(), True),\n",
       " StructField('pickup_datetime', TimestampType(), True),\n",
       " StructField('dropoff_datetime', TimestampType(), True),\n",
       " StructField('passenger_count', IntegerType(), True),\n",
       " StructField('pickup_longitude', DoubleType(), True),\n",
       " StructField('pickup_latitude', DoubleType(), True),\n",
       " StructField('dropoff_longitude', DoubleType(), True),\n",
       " StructField('dropoff_latitude', DoubleType(), True),\n",
       " StructField('store_and_fwd_flag', StringType(), True),\n",
       " StructField('trip_duration', IntegerType(), True)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schema = [StructField(i,j,True) for i,j in zip(df.columns,dtypes)]\n",
    "data_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', StringType(), True), StructField('vendor_id', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('passenger_count', IntegerType(), True), StructField('pickup_longitude', DoubleType(), True), StructField('pickup_latitude', DoubleType(), True), StructField('dropoff_longitude', DoubleType(), True), StructField('dropoff_latitude', DoubleType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('trip_duration', IntegerType(), True)])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_struct = StructType(fields =data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|id2875421|        2|2016-03-14 17:24:55|2016-03-14 17:32:30|              1| -73.9821548461914| 40.76793670654297|-73.96463012695312|40.765602111816406|                 N|          455|\n",
      "|id2377394|        1|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|-73.98041534423828|40.738563537597656|-73.99948120117188| 40.73115158081055|                 N|          663|\n",
      "|id3858529|        2|2016-01-19 11:35:24|2016-01-19 12:10:48|              1| -73.9790267944336|40.763938903808594|-74.00533294677734|40.710086822509766|                 N|         2124|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(r'.\\nyc-taxi-trip-duration\\train.csv',\n",
    "                                           schema = final_struct,header=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- trip_duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting portion / grouping of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|vendor_id|    pickup_datetime|\n",
      "+---------+-------------------+\n",
      "|        2|2016-03-14 17:24:55|\n",
      "|        1|2016-06-12 00:43:35|\n",
      "|        2|2016-01-19 11:35:24|\n",
      "|        2|2016-04-06 19:32:31|\n",
      "|        2|2016-03-26 13:30:55|\n",
      "+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['vendor_id','pickup_datetime']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------------+\n",
      "|vendor_id|    pickup_datetime|passenger_count|\n",
      "+---------+-------------------+---------------+\n",
      "|        2|2016-06-24 08:09:21|              9|\n",
      "|        2|2016-01-01 01:15:20|              8|\n",
      "+---------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"passenger_count > 7\").select(['vendor_id','pickup_datetime',\n",
    "                                         'passenger_count']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------------+\n",
      "|vendor_id|    pickup_datetime|passenger_count|\n",
      "+---------+-------------------+---------------+\n",
      "|        2|2016-06-24 08:09:21|              9|\n",
      "|        2|2016-01-01 01:15:20|              8|\n",
      "+---------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['passenger_count'] > 7).select(['vendor_id','pickup_datetime',\n",
    "                                         'passenger_count']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- & (and), |(or) ans ~(not) can be used to combine filter logics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By createOrReplaceTempView using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"Trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+-----------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|  pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+-----------------+------------------+------------------+------------------+-------------+\n",
      "|id0801584|        2|2016-01-30 22:01:40|2016-01-30 22:09:03|              6|-73.98285675048828|40.74219512939453|-73.99208068847656|40.749183654785156|                 N|          443|\n",
      "|id0675800|        2|2016-02-15 09:25:15|2016-02-15 09:35:49|              6|-73.97775268554688|40.75463104248047|-74.00167846679688| 40.75642013549805|                 N|          634|\n",
      "|id0913838|        2|2016-02-03 16:22:50|2016-02-03 16:37:20|              5|-73.97833251953125|40.75407028198242|-73.95973205566406| 40.77275848388672|                 N|          870|\n",
      "|id3508035|        2|2016-06-06 20:13:56|2016-06-06 20:45:20|              6|-73.78813171386719|40.64146041870117|-73.97177124023438| 40.74940872192383|                 N|         1884|\n",
      "|id2135557|        2|2016-05-25 13:19:56|2016-05-25 13:27:50|              5|-73.97028350830078|40.76483154296875|-73.95757293701172| 40.78247833251953|                 N|          474|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+-----------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_portion = spark.sql(\" Select * from Trips where passenger_count > 4\")\n",
    "df_portion.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### column manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.withColumn(\"NewColumn\",df['col'] operations).show()\n",
    "\n",
    "#df.withColumn(\"NewColumn\", (df['dropoff_datetime']-df['pickup_datetime'])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy & Aggregate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev,max,min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------------+--------------------+----------------------+---------------------+\n",
      "|vendor_id|avg(passenger_count)|avg(pickup_longitude)|avg(pickup_latitude)|avg(dropoff_longitude)|avg(dropoff_latitude)|\n",
      "+---------+--------------------+---------------------+--------------------+----------------------+---------------------+\n",
      "|        1|               -74.0|    40.75086441572701|  -73.97345260053869|     40.75172198288115|                 NULL|\n",
      "|        2|               -74.0|    40.75098219127021|  -73.97346282851106|    40.751897855869316|                 NULL|\n",
      "+---------+--------------------+---------------------+--------------------+----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"vendor_id\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------------------+--------------------+\n",
      "|vendor_id|count(store_and_fwd_flag)|max(pickup_longitude)|avg(passenger_count)|\n",
      "+---------+-------------------------+---------------------+--------------------+\n",
      "|        1|                        0|   42.814937591552734|               -74.0|\n",
      "|        2|                        0|      41.670166015625|               -74.0|\n",
      "+---------+-------------------------+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"vendor_id\").agg({'passenger_count':'mean',\n",
    "                             'pickup_longitude':'max',\n",
    "                             'store_and_fwd_flag':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+----------------+---------------+------------------+------------------+------------------+----------------+------------------+\n",
      "|       id|vendor_id|    pickup_datetime|dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|dropoff_latitude|store_and_fwd_flag|\n",
      "+---------+---------+-------------------+----------------+---------------+------------------+------------------+------------------+----------------+------------------+\n",
      "|id0621643|        2|2016-01-01 00:00:22|            NULL|           NULL|40.716880798339844|-73.96932983398438|40.769378662109375|            NULL|              NULL|\n",
      "|id1384355|        1|2016-01-01 00:00:28|            NULL|           NULL| 40.73356246948242|-73.85426330566406|40.891788482666016|            NULL|              NULL|\n",
      "|id2568735|        1|2016-01-01 00:01:24|            NULL|           NULL|40.759864807128906|-73.87660217285156| 40.74866485595703|            NULL|              NULL|\n",
      "+---------+---------+-------------------+----------------+---------------+------------------+------------------+------------------+----------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('pickup_datetime').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+----------------+---------------+-----------------+------------------+------------------+----------------+------------------+\n",
      "|       id|vendor_id|    pickup_datetime|dropoff_datetime|passenger_count| pickup_longitude|   pickup_latitude| dropoff_longitude|dropoff_latitude|store_and_fwd_flag|\n",
      "+---------+---------+-------------------+----------------+---------------+-----------------+------------------+------------------+----------------+------------------+\n",
      "|id3004672|        1|2016-06-30 23:59:58|            NULL|           NULL|40.73202896118164|-73.99017333984375| 40.75667953491211|            NULL|              NULL|\n",
      "|id3505355|        1|2016-06-30 23:59:53|            NULL|           NULL|40.67999267578125|-73.95980834960938| 40.65540313720703|            NULL|              NULL|\n",
      "|id1217141|        1|2016-06-30 23:59:47|            NULL|           NULL|40.73758316040039|-73.98616027832031|40.729522705078125|            NULL|              NULL|\n",
      "+---------+---------+-------------------+----------------+---------------+-----------------+------------------+------------------+----------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['pickup_datetime'].desc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.na.drop()\n",
    "#df.na.fill(\"Value\",subset=[\"cilumns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth,hour,dayofyear,\n",
    "                                   month,year,weekofyear,dayofweek,\n",
    "                                   date_format,format_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|dayofweek(pickup_datetime)|\n",
      "+--------------------------+\n",
      "|                         5|\n",
      "|                         5|\n",
      "|                         5|\n",
      "+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofweek(df['pickup_datetime'])).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.withColumn(\"Dayofweek\",dayofweek(df['pickup_datetime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+---------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|Dayofweek|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+---------+\n",
      "|id2478415|        2|2016-04-12 18:18:10|2016-04-12 18:21:27|              5|-73.96436309814453|40.767677307128906|-73.96813201904297| 40.76789093017578|                 N|          197|        3|\n",
      "|id0929764|        1|2016-05-26 08:05:21|2016-05-26 08:10:51|              1|-73.98748016357422|40.738887786865234|-73.97865295410156| 40.75092697143555|                 N|          330|        5|\n",
      "|id1697121|        2|2016-05-07 04:06:13|2016-05-07 04:24:41|              1| -73.9748764038086| 40.74184036254883|-73.89946746826172|40.697818756103516|                 N|         1108|        7|\n",
      "|id2687244|        2|2016-01-22 22:28:18|2016-01-22 22:40:31|              1|-73.99996185302734| 40.74324035644531|-73.98755645751953| 40.72885513305664|                 N|          733|        6|\n",
      "|id3097068|        2|2016-03-04 23:49:28|2016-03-05 00:03:10|              1|-73.97688293457031| 40.74751663208008|-74.00253295898438|  40.7607307434082|                 N|          822|        6|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.sample(fraction=0.00001).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|Dayofweek|Avg passenger count|\n",
      "+---------+-------------------+\n",
      "|        1|              1.717|\n",
      "|        6|              1.662|\n",
      "|        3|              1.636|\n",
      "|        5|              1.638|\n",
      "|        4|              1.633|\n",
      "|        7|              1.729|\n",
      "|        2|              1.634|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.groupBy('Dayofweek').mean()\\\n",
    "    .select(['Dayofweek',\n",
    "            format_number('avg(passenger_count)',3).alias(\"Avg passenger count\")\n",
    "             ]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17876\\1396537375.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
